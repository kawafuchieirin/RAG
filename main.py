"""
LangChain ã‚’ä½¿ã£ãŸ RAG (Retrieval-Augmented Generation) ã‚µãƒ³ãƒ—ãƒ«
"""

import os
import sys
from pathlib import Path
from typing import List, Dict, Any
from dotenv import load_dotenv

# LangChain ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_anthropic import ChatAnthropic
from langchain_chroma import Chroma
from langchain.chains import RetrievalQA
from langchain.docstore.document import Document
from langchain.prompts import PromptTemplate
from langchain_community.embeddings import HuggingFaceEmbeddings

# ç’°å¢ƒå¤‰æ•°ã®èª­ã¿è¾¼ã¿
load_dotenv()

class RAGSystem:
    """RAG ã‚·ã‚¹ãƒ†ãƒ ã®ãƒ¡ã‚¤ãƒ³ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, provider: str = "claude", openai_api_key: str = None, anthropic_api_key: str = None):
        """
        RAG ã‚·ã‚¹ãƒ†ãƒ ã®åˆæœŸåŒ–
        
        Args:
            provider: LLM ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ ("claude" ã¾ãŸã¯ "openai")
            openai_api_key: OpenAI API ã‚­ãƒ¼
            anthropic_api_key: Anthropic API ã‚­ãƒ¼
        """
        self.provider = provider.lower()
        
        if self.provider == "claude":
            # Claude/Anthropic ã‚’ä½¿ç”¨
            self.anthropic_api_key = anthropic_api_key or os.getenv("ANTHROPIC_API_KEY")
            if not self.anthropic_api_key:
                raise ValueError("ANTHROPIC_API_KEY ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
                
            # HuggingFace embeddings ã‚’ä½¿ç”¨ï¼ˆAnthropic ã¯åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«ã‚’æä¾›ã—ã¦ã„ãªã„ãŸã‚ï¼‰
            self.embeddings = HuggingFaceEmbeddings(
                model_name="sentence-transformers/all-MiniLM-L6-v2",
                model_kwargs={'device': 'cpu'},
                encode_kwargs={'normalize_embeddings': False}
            )
            
            self.llm = ChatAnthropic(
                model="claude-3-haiku-20240307",
                temperature=0,
                anthropic_api_key=self.anthropic_api_key
            )
            
        elif self.provider == "openai":
            # OpenAI ã‚’ä½¿ç”¨ï¼ˆå¾“æ¥ã®å®Ÿè£…ï¼‰
            self.openai_api_key = openai_api_key or os.getenv("OPENAI_API_KEY")
            if not self.openai_api_key:
                raise ValueError("OPENAI_API_KEY ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
                
            self.embeddings = OpenAIEmbeddings(
                model="text-embedding-3-small",
                openai_api_key=self.openai_api_key
            )
            
            self.llm = ChatOpenAI(
                model_name="gpt-3.5-turbo",
                temperature=0,
                openai_api_key=self.openai_api_key
            )
        else:
            raise ValueError(f"ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ãªã„ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã§ã™: {provider}")
        
        # ãƒ†ã‚­ã‚¹ãƒˆåˆ†å‰²å™¨ã®è¨­å®š
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=200,
            length_function=len,
            separators=["\n\n", "\n", " ", ""]
        )
        
        # Chroma ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã®åˆæœŸåŒ–
        self.vectorstore = None
        self.qa_chain = None
        
    def load_documents(self, file_path: str) -> List[Document]:
        """
        ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’èª­ã¿è¾¼ã¿
        
        Args:
            file_path: ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
            
        Returns:
            Document ã®ãƒªã‚¹ãƒˆ
        """
        try:
            with open(file_path, 'r', encoding='utf-8') as file:
                content = file.read()
                
            # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®ä½œæˆ
            doc = Document(
                page_content=content,
                metadata={"source": file_path}
            )
            
            return [doc]
            
        except Exception as e:
            print(f"ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            return []
    
    def split_documents(self, documents: List[Document]) -> List[Document]:
        """
        ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’å°ã•ãªãƒãƒ£ãƒ³ã‚¯ã«åˆ†å‰²
        
        Args:
            documents: åˆ†å‰²ã™ã‚‹ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®ãƒªã‚¹ãƒˆ
            
        Returns:
            åˆ†å‰²ã•ã‚ŒãŸãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®ãƒªã‚¹ãƒˆ
        """
        return self.text_splitter.split_documents(documents)
    
    def create_vectorstore(self, documents: List[Document]) -> None:
        """
        ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã‚’ä½œæˆã—ã¦ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’ä¿å­˜
        
        Args:
            documents: ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹åŒ–ã™ã‚‹ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®ãƒªã‚¹ãƒˆ
        """
        try:
            self.vectorstore = Chroma.from_documents(
                documents=documents,
                embedding=self.embeddings,
                persist_directory="./chroma_db"
            )
            print(f"ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã‚’ä½œæˆã—ã¾ã—ãŸã€‚ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ•°: {len(documents)}")
            
        except Exception as e:
            print(f"ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ä½œæˆã‚¨ãƒ©ãƒ¼: {e}")
            raise
    
    def create_qa_chain(self) -> None:
        """
        QA ãƒã‚§ãƒ¼ãƒ³ã‚’ä½œæˆ
        """
        if not self.vectorstore:
            raise ValueError("ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ãŒä½œæˆã•ã‚Œã¦ã„ã¾ã›ã‚“")
            
        # ã‚«ã‚¹ã‚¿ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã®ä½œæˆ
        prompt_template = """ä»¥ä¸‹ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’ä½¿ç”¨ã—ã¦è³ªå•ã«ç­”ãˆã¦ãã ã•ã„ã€‚
        ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã«ãªã„æƒ…å ±ã«ã¤ã„ã¦ã¯ã€Œã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã«æƒ…å ±ãŒã‚ã‚Šã¾ã›ã‚“ã€ã¨å›ç­”ã—ã¦ãã ã•ã„ã€‚
        
        ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ: {context}
        
        è³ªå•: {question}
        
        å›ç­”:"""
        
        PROMPT = PromptTemplate(
            template=prompt_template,
            input_variables=["context", "question"]
        )
        
        # RetrievalQA ãƒã‚§ãƒ¼ãƒ³ã®ä½œæˆ
        self.qa_chain = RetrievalQA.from_chain_type(
            llm=self.llm,
            chain_type="stuff",
            retriever=self.vectorstore.as_retriever(search_kwargs={"k": 3}),
            chain_type_kwargs={"prompt": PROMPT},
            return_source_documents=True
        )
    
    def ask_question(self, question: str) -> Dict[str, Any]:
        """
        è³ªå•ã«å›ç­”
        
        Args:
            question: è³ªå•æ–‡
            
        Returns:
            å›ç­”ã¨ã‚½ãƒ¼ã‚¹æ–‡æ›¸ã‚’å«ã‚€è¾æ›¸
        """
        if not self.qa_chain:
            raise ValueError("QA ãƒã‚§ãƒ¼ãƒ³ãŒä½œæˆã•ã‚Œã¦ã„ã¾ã›ã‚“")
            
        try:
            result = self.qa_chain({"query": question})
            
            return {
                "answer": result["result"],
                "source_documents": result["source_documents"]
            }
            
        except Exception as e:
            print(f"è³ªå•å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
            return {"answer": "ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ", "source_documents": []}

def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    print("ğŸš€ RAG ã‚·ã‚¹ãƒ†ãƒ ã‚’é–‹å§‹ã—ã¦ã„ã¾ã™...")
    
    # ä½¿ç”¨ã™ã‚‹ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã‚’æŒ‡å®šï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯ Claudeï¼‰
    provider = "claude"  # "claude" ã¾ãŸã¯ "openai"
    
    # API ã‚­ãƒ¼ã®ç¢ºèª
    if provider == "claude":
        api_key = os.getenv("ANTHROPIC_API_KEY")
        if not api_key:
            print("âŒ ANTHROPIC_API_KEY ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
            print("ğŸ’¡ ä»¥ä¸‹ã®æ–¹æ³•ã§ API ã‚­ãƒ¼ã‚’è¨­å®šã—ã¦ãã ã•ã„ï¼š")
            print("   1. .env ãƒ•ã‚¡ã‚¤ãƒ«ã« ANTHROPIC_API_KEY=your_key_here ã‚’è¿½åŠ ")
            print("   2. ç’°å¢ƒå¤‰æ•°ã¨ã—ã¦ export ANTHROPIC_API_KEY=your_key_here")
            return
    else:
        api_key = os.getenv("OPENAI_API_KEY")
        if not api_key:
            print("âŒ OPENAI_API_KEY ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
            print("ğŸ’¡ ä»¥ä¸‹ã®æ–¹æ³•ã§ API ã‚­ãƒ¼ã‚’è¨­å®šã—ã¦ãã ã•ã„ï¼š")
            print("   1. .env ãƒ•ã‚¡ã‚¤ãƒ«ã« OPENAI_API_KEY=your_key_here ã‚’è¿½åŠ ")
            print("   2. ç’°å¢ƒå¤‰æ•°ã¨ã—ã¦ export OPENAI_API_KEY=your_key_here")
            return
    
    try:
        # RAG ã‚·ã‚¹ãƒ†ãƒ ã®åˆæœŸåŒ–
        rag_system = RAGSystem(provider=provider)
        
        # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®èª­ã¿è¾¼ã¿
        print("ğŸ“„ ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’èª­ã¿è¾¼ã‚“ã§ã„ã¾ã™...")
        documents = rag_system.load_documents("data/example.txt")
        
        if not documents:
            print("âŒ ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ")
            return
            
        # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®åˆ†å‰²
        print("âœ‚ï¸ ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’åˆ†å‰²ã—ã¦ã„ã¾ã™...")
        split_docs = rag_system.split_documents(documents)
        print(f"ğŸ“ {len(split_docs)} å€‹ã®ãƒãƒ£ãƒ³ã‚¯ã«åˆ†å‰²ã—ã¾ã—ãŸ")
        
        # ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã®ä½œæˆ
        print("ğŸ” ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã‚’ä½œæˆã—ã¦ã„ã¾ã™...")
        rag_system.create_vectorstore(split_docs)
        
        # QA ãƒã‚§ãƒ¼ãƒ³ã®ä½œæˆ
        print("ğŸ”— QA ãƒã‚§ãƒ¼ãƒ³ã‚’ä½œæˆã—ã¦ã„ã¾ã™...")
        rag_system.create_qa_chain()
        
        print("\nâœ… RAG ã‚·ã‚¹ãƒ†ãƒ ã®æº–å‚™ãŒå®Œäº†ã—ã¾ã—ãŸï¼")
        print("=" * 50)
        
        # ã‚µãƒ³ãƒ—ãƒ«è³ªå•ã®å®Ÿè¡Œ
        sample_questions = [
            "LangChain ã¨ã¯ä½•ã§ã™ã‹ï¼Ÿ",
            "RAG ã®åˆ©ç‚¹ã‚’æ•™ãˆã¦ãã ã•ã„",
            "Chroma ã®ç‰¹å¾´ã¯ä½•ã§ã™ã‹ï¼Ÿ",
            "OpenAI Embeddings ã«ã¯ã©ã®ã‚ˆã†ãªãƒ¢ãƒ‡ãƒ«ãŒã‚ã‚Šã¾ã™ã‹ï¼Ÿ"
        ]
        
        for i, question in enumerate(sample_questions, 1):
            print(f"\nğŸ¤” è³ªå• {i}: {question}")
            print("-" * 30)
            
            result = rag_system.ask_question(question)
            print(f"ğŸ¤– å›ç­”: {result['answer']}")
            
            if result['source_documents']:
                print(f"ğŸ“š å‚ç…§å…ƒ: {len(result['source_documents'])} å€‹ã®æ–‡æ›¸")
                for j, doc in enumerate(result['source_documents'][:2]):  # æœ€åˆã®2ã¤ã®ã¿è¡¨ç¤º
                    print(f"   - æ–‡æ›¸ {j+1}: {doc.page_content[:100]}...")
            
            print()
        
        print("=" * 50)
        print("ğŸ‰ å‹•ä½œç¢ºèªãŒå®Œäº†ã—ã¾ã—ãŸï¼")
        
        # ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãƒ¢ãƒ¼ãƒ‰
        print("\nğŸ’¬ ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãƒ¢ãƒ¼ãƒ‰ã«å…¥ã‚Šã¾ã™ï¼ˆ'quit' ã§çµ‚äº†ï¼‰")
        while True:
            user_question = input("\nè³ªå•ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„: ")
            if user_question.lower() in ['quit', 'exit', 'çµ‚äº†']:
                break
                
            result = rag_system.ask_question(user_question)
            print(f"ğŸ¤– å›ç­”: {result['answer']}")
            
    except Exception as e:
        print(f"âŒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        return

if __name__ == "__main__":
    main()